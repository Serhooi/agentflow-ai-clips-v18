#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
"""
import asyncio
import concurrent.futures
import time
from typing import Dict, Optional, List

async def optimized_analyze_video_task(task_id: str, video_id: str, auto_emoji: bool = False):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    try:
        start_time = time.time()
        logger.info(f"üöÄ –ù–∞—á–∞—Ç –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ: {video_id}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        analysis_tasks[task_id]["progress"] = 5
        
        # –ù–∞—Ö–æ–¥–∏–º –≤–∏–¥–µ–æ —Ñ–∞–π–ª
        video_files = [f for f in os.listdir(Config.UPLOAD_DIR) if f.startswith(video_id)]
        if not video_files:
            raise Exception("–í–∏–¥–µ–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        video_path = os.path.join(Config.UPLOAD_DIR, video_files[0])
        audio_path = os.path.join(Config.AUDIO_DIR, f"{video_id}.wav")
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø 1: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            audio_future = executor.submit(extract_audio_optimized, video_path, audio_path)
            duration_future = executor.submit(get_video_duration_fast, video_path)
            
            # –ñ–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            audio_success = audio_future.result()
            video_duration = duration_future.result()
            
            if not audio_success:
                raise Exception("–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ")
        
        analysis_tasks[task_id]["progress"] = 25
        logger.info(f"‚ö° –ê—É–¥–∏–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ –∑–∞ {time.time() - start_time:.1f}s")
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø 2: –ë—ã—Å—Ç—Ä–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        transcript_start = time.time()
        transcript_result = await fast_transcribe_audio(audio_path, auto_emoji, video_duration)
        if not transcript_result:
            raise Exception("–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
        
        analysis_tasks[task_id]["progress"] = 60
        logger.info(f"‚ö° –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time() - transcript_start:.1f}s")
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø 3: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –∏ –∞–Ω–∞–ª–∏–∑
        if "words" in transcript_result:
            transcript_text = " ".join([word["word"] for word in transcript_result["words"]])
            transcript_words = transcript_result["words"]
        else:
            transcript_text = transcript_result.get("text", "")
            transcript_words = []
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ ChatGPT –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        analysis_start = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
            analysis_future = executor.submit(analyze_with_chatgpt_fast, transcript_text, video_duration)
            preprocessing_future = executor.submit(preprocess_transcript_data, transcript_text, video_duration)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            analysis_result = analysis_future.result()
            preprocessing_data = preprocessing_future.result()
        
        analysis_tasks[task_id]["progress"] = 90
        logger.info(f"‚ö° –ê–Ω–∞–ª–∏–∑ ChatGPT –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {time.time() - analysis_start:.1f}s")
        
        # Fallback –µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è
        if not analysis_result:
            analysis_result = create_fallback_highlights_fast(video_duration, preprocessing_data)
        
        # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        total_time = time.time() - start_time
        analysis_tasks[task_id].update({
            "status": "completed",
            "progress": 100,
            "completed_at": datetime.now(),
            "processing_time": total_time,
            "result": {
                "highlights": analysis_result["highlights"],
                "transcript": transcript_words,
                "video_duration": video_duration
            }
        })
        
        logger.info(f"üéâ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_time:.1f}s (–±—ã–ª–æ ~{total_time*2:.1f}s)")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ {video_id}: {e}")
        analysis_tasks[task_id].update({
            "status": "failed",
            "error": str(e),
            "completed_at": datetime.now()
        })

def extract_audio_optimized(video_path: str, audio_path: str) -> bool:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    try:
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        cmd = [
            'ffmpeg', '-i', video_path,
            '-vn',  # –ë–µ–∑ –≤–∏–¥–µ–æ
            '-acodec', 'mp3',
            '-ar', '16000',  # –ù–∏–∑–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–ª—è Whisper
            '-ac', '1',  # –ú–æ–Ω–æ
            '-ab', '32k',  # –ï—â–µ –Ω–∏–∂–µ –±–∏—Ç—Ä–µ–π—Ç –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            '-threads', '2',  # –ë–æ–ª—å—à–µ –ø–æ—Ç–æ–∫–æ–≤
            '-preset', 'ultrafast',  # –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –ø—Ä–µ—Å–µ—Ç
            '-y', audio_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=120)
        return os.path.exists(audio_path)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—É–¥–∏–æ: {e}")
        return False

def get_video_duration_fast(video_path: str) -> float:
    """–ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ"""
    try:
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º ffprobe —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        cmd = ['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration', '-of', 'csv=p=0', video_path]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=10)
        return float(result.stdout.strip())
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")
        return 60.0

async def fast_transcribe_audio(audio_path: str, auto_emoji: bool = False, video_duration: float = 60.0) -> Optional[Dict]:
    """–ë—ã—Å—Ç—Ä–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
    try:
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π
        cache_key = f"transcript_{os.path.basename(audio_path)}_{auto_emoji}"
        if REDIS_AVAILABLE:
            try:
                cached_result = redis_client.get(cache_key)
                if cached_result:
                    logger.info("‚ö° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
                    return json.loads(cached_result)
            except:
                pass
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–∂–∏–º–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª –µ—Å–ª–∏ –æ–Ω –±–æ–ª—å—à–æ–π
        file_size = os.path.getsize(audio_path)
        if file_size > 10 * 1024 * 1024:  # –ë–æ–ª—å—à–µ 10MB
            compressed_path = audio_path.replace('.wav', '_compressed.wav')
            if compress_audio_for_whisper(audio_path, compressed_path):
                audio_path = compressed_path
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        with open(audio_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                response_format="verbose_json",
                timestamp_granularities=["word"],
                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                prompt="Include filler words: um, uh, yeah, like, so, well, actually, right, okay."
            )
            
        result = transcript.model_dump() if hasattr(transcript, 'model_dump') else dict(transcript)
        
        # –ë—ã—Å—Ç—Ä–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
        if 'words' in result:
            result['words'] = enhance_filler_words_fast(result['words'])
            
            if auto_emoji:
                result['words'] = addEmojisToText(result['words'], video_duration)
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if REDIS_AVAILABLE:
            try:
                redis_client.setex(cache_key, 3600, json.dumps(result))  # –ö—ç—à –Ω–∞ 1 —á–∞—Å
            except:
                pass
        
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
        return None

def compress_audio_for_whisper(input_path: str, output_path: str) -> bool:
    """–°–∂–∏–º–∞–µ—Ç –∞—É–¥–∏–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è Whisper"""
    try:
        cmd = [
            'ffmpeg', '-i', input_path,
            '-ar', '16000',  # Whisper –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
            '-ac', '1',      # –ú–æ–Ω–æ
            '-ab', '16k',    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∏—Ç—Ä–µ–π—Ç
            '-y', output_path
        ]
        subprocess.run(cmd, capture_output=True, check=True, timeout=60)
        return os.path.exists(output_path)
    except:
        return False

def enhance_filler_words_fast(words: List[Dict]) -> List[Dict]:
    """–ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å—Ç–∞–≤–Ω—ã—Ö —Å–ª–æ–≤"""
    # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏
    quick_corrections = {
        'um': ['uhm', 'umm'], 'uh': ['uhh'], 'yeah': ['yah', 'yea'],
        'like': ['lyk'], 'okay': ['ok'], 'right': ['rite']
    }
    
    for word in words:
        word_text = word.get('word', '').strip().lower()
        for correct, variations in quick_corrections.items():
            if word_text in variations:
                word['word'] = correct
                break
    
    return words

def analyze_with_chatgpt_fast(transcript_text: str, video_duration: float) -> Optional[Dict]:
    """–ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è –∞–Ω–∞–ª–∏–∑–∞ ChatGPT —Å —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º"""
    try:
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        target_clips = min(3, max(1, int(video_duration / 60)))  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞
        
        prompt = f"""Find {target_clips} best moments in this {video_duration:.0f}s video for short clips.

Transcript: {transcript_text[:2000]}...  

Return JSON with highlights array. Each highlight needs:
- start_time, end_time (in seconds, 0-{video_duration:.0f})
- title (3-5 words, English)
- description (why it's interesting)
- Duration: 40-80 seconds each

Focus on: valuable insights, funny moments, key information, emotional peaks.

JSON format:
{{"highlights": [{{"start_time": 0, "end_time": 60, "title": "Key Insight", "description": "Main valuable moment"}}]}}"""

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, –±—ã—Å—Ç—Ä–µ–µ –æ—Ç–≤–µ—Ç
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,  # –ú–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
            temperature=0.3  # –ú–µ–Ω—å—à–µ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏, –±–æ–ª—å—à–µ —Å–∫–æ—Ä–æ—Å—Ç–∏
        )
        
        content = response.choices[0].message.content.strip()
        if content.startswith('```json'):
            content = content[7:]
        if content.endswith('```'):
            content = content[:-3]
        
        result = json.loads(content.strip())
        highlights = result.get("highlights", [])
        
        # –ë—ã—Å—Ç—Ä–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        for highlight in highlights:
            duration = highlight["end_time"] - highlight["start_time"]
            if duration < 40:
                highlight["end_time"] = min(highlight["start_time"] + 40, video_duration)
            elif duration > 80:
                highlight["end_time"] = highlight["start_time"] + 80
        
        return {"highlights": highlights}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ ChatGPT: {e}")
        return None

def preprocess_transcript_data(transcript_text: str, video_duration: float) -> Dict:
    """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞"""
    return {
        "word_count": len(transcript_text.split()),
        "duration": video_duration,
        "content_type": "general"  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
    }

def create_fallback_highlights_fast(video_duration: float, preprocessing_data: Dict) -> Dict:
    """–ë—ã—Å—Ç—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ fallback —Ö–∞–π–ª–∞–π—Ç–æ–≤"""
    clips_count = min(2, max(1, int(video_duration / 60)))
    highlights = []
    
    segment_duration = video_duration / clips_count
    for i in range(clips_count):
        start = i * segment_duration
        end = min(start + 50, video_duration)  # 50 —Å–µ–∫—É–Ω–¥ –∫–ª–∏–ø
        
        highlights.append({
            "start_time": start,
            "end_time": end,
            "title": f"Moment {i+1}",
            "description": "Auto-generated highlight"
        })
    
    return {"highlights": highlights}